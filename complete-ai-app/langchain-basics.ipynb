{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "OPENAI_EMBEDDING_VERSION = os.getenv(\"OPENAI_EMBEDDING_VERSION\")\n",
    "\n",
    "#init Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(engine=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who directed movie RRR'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt1 = \"\"\"Who directed movie {movieName}\"\"\"\n",
    "\n",
    "promptTemplate1 = PromptTemplate(\n",
    "    input_variables = [\"movieName\"],\n",
    "    template = prompt1\n",
    ")\n",
    "\n",
    "promptTemplate1.format(movieName = \"RRR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who directed movie Titanic'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loadedPrompt = load_prompt(\"./prompt.json\")\n",
    "\n",
    "loadedPrompt.format(movieName = \"Titanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "The movie RRR is directed by S. S. Rajamouli. \n",
      "\n",
      "S. S. Rajamouli was born on October 10, 1973.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "movie_prompt = \"\"\"Who directed movie {movieName}\"\"\"\n",
    "\n",
    "movie_promptTemplate = PromptTemplate(\n",
    "    input_variables = [\"movieName\"],\n",
    "    template = movie_prompt\n",
    ")\n",
    "\n",
    "movie_chain = LLMChain(llm=llm, prompt=movie_promptTemplate, output_key=\"output\")\n",
    "\n",
    "director_prompt = \"\"\"When was the director mentioned in the movie '{output}' born?\"\"\"\n",
    "\n",
    "director_promptTemplate = PromptTemplate(\n",
    "    input_variables=[\"output\"],\n",
    "    template=director_prompt,\n",
    ")\n",
    "\n",
    "director_chain = LLMChain(llm=llm, prompt=director_promptTemplate, output_key=\"result\")\n",
    "\n",
    "sequence = SequentialChain(\n",
    "    chains = [movie_chain, director_chain],\n",
    "    input_variables=[\"movieName\"],\n",
    "    output_variables=[\"output\", \"result\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = sequence({\n",
    "    \"movieName\": \"RRR\"\n",
    "})\n",
    "\n",
    "print(response.get(\"output\"), response.get(\"result\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key = os.getenv(\"SERPER_API_KEY\"))\n",
    "llm_math = LLMMathChain.from_llm(llm=llm)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func = search.run,\n",
    "        description = '''useful for when you need to answer questions about current events,\n",
    "            getting flights information like cheapest flights and best flights,\n",
    "            getting weather forecast infomation like temperature and humidity\n",
    "            '''\n",
    "    ),\n",
    "    Tool(\n",
    "        name = 'Calculator',\n",
    "        func = llm_math.run,\n",
    "        description = 'Useful for when you need to answer questions about math.'\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature in New York and then convert it\n",
      "Action: Search\n",
      "Action Input: Weather in New York\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSun 16 | Night ... Thunderstorms early, then partly cloudy after midnight. Potential for heavy rainfall. Low 73F. Winds light and variable. Chance of rain 60%.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert the temperature from fahrenheit to celsius\n",
      "Action: Calculator\n",
      "Action Input: 73F to Celsius\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 22.77777777777778\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The temperature in New York is 22.78°C.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The temperature in New York is 22.78°C.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How is the weather in New York? Convert its temperature from fahrenheit to celsius\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hey! My name is Akash\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Akash, my name is AI. It's nice to meet you. What brings you here today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Hey! My name is Akash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey! My name is Akash\n",
      "AI:  Hi Akash, my name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: duh! what did I say my name was?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said your name is Akash. Is there something else I can help you with?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"duh! what did I say my name was?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering new chain with inputs:  {'movieName': 'Titanic'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWho directed movie Titanic\u001b[0m\n",
      "Chain ended with: {'text': '\\n\\nJames Cameron directed the movie Titanic.'}\n",
      "Completed chain: 1\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nJames Cameron directed the movie Titanic.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import LLMResult, HumanMessage\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "callbackPrompt = \"\"\"Who directed movie {movieName}\"\"\"\n",
    "\n",
    "callbackPromptTemplate = PromptTemplate(\n",
    "    input_variables = [\"movieName\"],\n",
    "    template = prompt1\n",
    ")\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    count: int = 0\n",
    "    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:\n",
    "        print('Entering new chain with inputs: ', inputs)\n",
    "        self.count += 1\n",
    "\n",
    "    def on_chain_end(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Chain ended with: {token}\")\n",
    "        print(f\"Completed chain: {self.count}\")\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        print(f\"LLM ended with: {response}\")\n",
    "\n",
    "callbackChain = LLMChain(llm=llm, prompt=callbackPromptTemplate, verbose=True, callbacks=[MyCustomHandler()])\n",
    "\n",
    "callbackChain.run(\"Titanic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
